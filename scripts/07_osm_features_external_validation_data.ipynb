{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original code from Jurgens Fourie, used for this study with minor adjustments. For the study by Fourie et al., please see: Fourie, J., Klingwort, J., and Gootzen. Y (2025). Rule-based transport mode classification in a smartphone-based travel survey. CBS Discussion Paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrosm import get_data\n",
    "from pyrosm import OSM\n",
    "import pandas as pd\n",
    "from shapely.wkt import loads\n",
    "import numpy as np\n",
    "from shapely.geometry import LineString, Point\n",
    "import geopandas as gpd\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from shapely import wkt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get OSM points of interest (POI's) for regions in Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_ns = OSM(\"niedersachsen-latest.osm.pbf\")\n",
    "osm_bw = OSM(\"baden-wuerttemberg-latest.osm.pbf\")\n",
    "osm_nw = OSM(\"nordrhein-westfalen-latest.osm.pbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom filter to fetch only public transport nodes for trains\n",
    "custom_filter = {'type': ['network'],\n",
    "                \"route\": ['train']\n",
    "                }\n",
    "\n",
    "# Read points of interest (POIs) with the custom filter\n",
    "train_routes_ns = osm_ns.get_pois(custom_filter=custom_filter)\n",
    "train_routes_bw = osm_bw.get_pois(custom_filter=custom_filter)\n",
    "train_routes_nw = osm_nw.get_pois(custom_filter=custom_filter)\n",
    "\n",
    "# keep only route data\n",
    "train_routes_ns = train_routes_ns[train_routes_ns['type']=='route']\n",
    "train_routes_bw = train_routes_bw[train_routes_bw['type']=='route']\n",
    "train_routes_nw = train_routes_nw[train_routes_nw['type']=='route']\n",
    "\n",
    "# Write the dataframe to a CSV file\n",
    "train_routes_ns.to_csv(\"data/osm/poi/osm_poi_train_routes_ns.csv\", index=False)\n",
    "train_routes_bw.to_csv(\"data/osm/poi/osm_poi_train_routes_bw.csv\", index=False)\n",
    "train_routes_nw.to_csv(\"data/osm/poi/osm_poi_train_routes_nw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tram routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_filter = {'type': ['network'],\n",
    "                \"route\": ['tram']\n",
    "                }\n",
    "\n",
    "tram_routes_ns = osm_ns.get_pois(custom_filter=custom_filter)\n",
    "tram_routes_bw = osm_bw.get_pois(custom_filter=custom_filter)\n",
    "tram_routes_nw = osm_nw.get_pois(custom_filter=custom_filter)\n",
    "\n",
    "tram_routes_ns = tram_routes_ns[tram_routes_ns['type']=='route']\n",
    "tram_routes_bw = tram_routes_bw[tram_routes_bw['type']=='route']\n",
    "tram_routes_nw = tram_routes_nw[tram_routes_nw['type']=='route']\n",
    "\n",
    "tram_routes_ns.to_csv(\"data/osm/poi/osm_poi_tram_routes_ns.csv\", index=False)\n",
    "tram_routes_bw.to_csv(\"data/osm/poi/osm_poi_tram_routes_bw.csv\", index=False)\n",
    "tram_routes_nw.to_csv(\"data/osm/poi/osm_poi_tram_routes_nw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bus routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_filter = {'type': ['network'],\n",
    "                \"route\": ['bus']\n",
    "                }\n",
    "\n",
    "bus_routes_ns = osm_ns.get_pois(custom_filter=custom_filter)\n",
    "bus_routes_bw = osm_bw.get_pois(custom_filter=custom_filter)\n",
    "bus_routes_nw = osm_nw.get_pois(custom_filter=custom_filter)\n",
    "\n",
    "bus_routes_ns = bus_routes_ns[bus_routes_ns['type']=='route']\n",
    "bus_routes_bw = bus_routes_bw[bus_routes_bw['type']=='route']\n",
    "bus_routes_nw = bus_routes_nw[bus_routes_nw['type']=='route']\n",
    "\n",
    "bus_routes_ns.to_csv(\"data/osm/poi/osm_poi_bus_routes_ns.csv\", index=False)\n",
    "bus_routes_bw.to_csv(\"data/osm/poi/osm_poi_bus_routes_bw.csv\", index=False)\n",
    "bus_routes_nw.to_csv(\"data/osm/poi/osm_poi_bus_routes_nw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metro routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subway routes are only in niedersachsen-westfalen\n",
    "custom_filter = {'type': ['network'],\n",
    "                \"route\": ['subway']\n",
    "                }\n",
    "\n",
    "# metro_routes_ns = osm_ns.get_pois(custom_filter=custom_filter)\n",
    "# metro_routes_bw = osm_bw.get_pois(custom_filter=custom_filter)\n",
    "metro_routes_nw = osm_nw.get_pois(custom_filter=custom_filter)\n",
    "\n",
    "# metro_routes_ns = metro_routes_ns[metro_routes_ns['type']=='route']\n",
    "# metro_routes_bw = metro_routes_bw[metro_routes_bw['type']=='route']\n",
    "metro_routes_nw = metro_routes_nw[metro_routes_nw['type']=='route']\n",
    "\n",
    "# metro_routes_ns.to_csv(\"osm_poi_metro_routes_ns.csv\", index=False)\n",
    "# metro_routes_bw.to_csv(\"osm_poi_metro_routes_bw.csv\", index=False)\n",
    "metro_routes_nw.to_csv(\"data/osm/poi/osm_poi_metro_routes_nw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subway routes are only in niedersachsen-westfalen\n",
    "custom_filter = {'type': ['network'],\n",
    "                \"route\": ['subway']\n",
    "                }\n",
    "\n",
    "metro_routes_ns = osm_ns.get_pois(custom_filter=custom_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bike routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_filter = {'type': ['network'],\n",
    "                \"route\": ['bicycle']\n",
    "                }\n",
    "\n",
    "bike_routes_ns = osm_ns.get_pois(custom_filter=custom_filter)\n",
    "bike_routes_bw = osm_bw.get_pois(custom_filter=custom_filter)\n",
    "bike_routes_nw = osm_nw.get_pois(custom_filter=custom_filter)\n",
    "\n",
    "bike_routes_ns = bike_routes_ns[bike_routes_ns['type']=='route']\n",
    "bike_routes_bw = bike_routes_bw[bike_routes_bw['type']=='route']\n",
    "bike_routes_nw = bike_routes_nw[bike_routes_nw['type']=='route']\n",
    "\n",
    "bike_routes_ns.to_csv(\"data/osm/poi/osm_poi_bike_routes_ns.csv\", index=False)\n",
    "bike_routes_bw.to_csv(\"data/osm/poi/osm_poi_bike_routes_bw.csv\", index=False)\n",
    "bike_routes_nw.to_csv(\"data/osm/poi/osm_poi_bike_routes_nw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_filter = {'public_transport': ['stop_position'],\n",
    "                \"railway\": ['station','halt']\n",
    "                }\n",
    "\n",
    "train_stops_ns = osm_ns.get_pois(custom_filter=custom_filter)\n",
    "train_stops_bw = osm_bw.get_pois(custom_filter=custom_filter)\n",
    "train_stops_nw = osm_nw.get_pois(custom_filter=custom_filter)\n",
    "\n",
    "train_stops_ns = train_stops_ns[train_stops_ns['osm_type'] == 'node']\n",
    "train_stops_bw = train_stops_bw[train_stops_bw['osm_type'] == 'node']\n",
    "train_stops_bw = train_stops_bw[train_stops_bw['osm_type'] == 'node']\n",
    "\n",
    "# Filter the DataFrame to include only specific values in the 'railway' column\n",
    "train_stops_ns = train_stops_ns[train_stops_ns['railway'].isin(['stop', 'station'])]\n",
    "train_stops_bw = train_stops_bw[train_stops_bw['railway'].isin(['stop', 'station'])]\n",
    "train_stops_nw = train_stops_nw[train_stops_nw['railway'].isin(['stop', 'station'])]\n",
    "\n",
    "train_stops_ns.to_csv(\"data/osm/poi/osm_poi_train_stops_ns.csv\", index=False)\n",
    "train_stops_bw.to_csv(\"data/osm/poi/osm_poi_train_stops_bw.csv\", index=False)\n",
    "train_stops_nw.to_csv(\"data/osm/poi/osm_poi_train_stops_nw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tram stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_filter_tram = { \n",
    "    \"railway\": [\"tram_stop\"]  # Specify tram as the railway type\n",
    "}\n",
    "\n",
    "tram_stops_ns = osm_ns.get_pois(custom_filter=custom_filter_tram)\n",
    "tram_stops_bw = osm_bw.get_pois(custom_filter=custom_filter_tram)\n",
    "tram_stops_nw = osm_nw.get_pois(custom_filter=custom_filter_tram)\n",
    "\n",
    "tram_stops_ns = tram_stops_ns[tram_stops_ns['osm_type'] == 'node']\n",
    "tram_stops_bw = tram_stops_bw[tram_stops_bw['osm_type'] == 'node']\n",
    "tram_stops_nw = tram_stops_nw[tram_stops_nw['osm_type'] == 'node']\n",
    "\n",
    "tram_stops_ns.to_csv(\"data/osm/poi/osm_poi_tram_stops_ns.csv\", index=False)\n",
    "tram_stops_bw.to_csv(\"data/osm/poi/osm_poi_tram_stops_bw.csv\", index=False)\n",
    "tram_stops_nw.to_csv(\"data/osm/poi/osm_poi_tram_stops_nw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bus stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_filter_bus = {\n",
    "    \"highway\": [\"bus_stop\"]  # Specify bus as the railway type\n",
    "}\n",
    "\n",
    "bus_stops_ns = osm_ns.get_pois(custom_filter=custom_filter_bus)\n",
    "bus_stops_bw = osm_bw.get_pois(custom_filter=custom_filter_bus)\n",
    "bus_stops_nw = osm_nw.get_pois(custom_filter=custom_filter_bus)\n",
    "\n",
    "bus_stops_ns = bus_stops_ns[bus_stops_ns['osm_type'] == 'node']\n",
    "bus_stops_bw = bus_stops_bw[bus_stops_bw['osm_type'] == 'node']\n",
    "bus_stops_nw = bus_stops_nw[bus_stops_nw['osm_type'] == 'node']\n",
    "\n",
    "bus_stops_ns.to_csv(\"data/osm/poi/osm_poi_bus_stops_ns.csv\", index=False)\n",
    "bus_stops_bw.to_csv(\"data/osm/poi/osm_poi_bus_stops_bw.csv\", index=False)\n",
    "bus_stops_nw.to_csv(\"data/osm/poi/osm_poi_bus_stops_nw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metro stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_filter_metro = {\n",
    "    \"railway\": [\"station\"],\n",
    "    \"station\": ['subway']\n",
    "}\n",
    "\n",
    "metro_stops_ns = osm_ns.get_pois(custom_filter=custom_filter_metro)\n",
    "metro_stops_bw = osm_bw.get_pois(custom_filter=custom_filter_metro)\n",
    "metro_stops_nw = osm_nw.get_pois(custom_filter=custom_filter_metro)\n",
    "\n",
    "metro_stops_ns = metro_stops_ns[metro_stops_ns['osm_type'] == 'node']\n",
    "metro_stops_bw = metro_stops_bw[metro_stops_bw['osm_type'] == 'node']\n",
    "metro_stops_nw = metro_stops_nw[metro_stops_nw['osm_type'] == 'node']\n",
    "\n",
    "metro_stops_ns.to_csv(\"data/osm/poi/osm_poi_metro_stops_ns.csv\", index=False)\n",
    "metro_stops_bw.to_csv(\"data/osm/poi/osm_poi_metro_stops_bw.csv\", index=False)\n",
    "metro_stops_nw.to_csv(\"data/osm/poi/osm_poi_metro_stops_nw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parking amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_filter = {\n",
    "    'amenity': ['parking', 'bicycle_parking', \n",
    "                'parking_entrance', 'parking_underground', \n",
    "                'bicycle_rental', 'fuel', 'charging_station']\n",
    "}\n",
    "\n",
    "parking_amenities_ns = osm_ns.get_pois(custom_filter=custom_filter)\n",
    "parking_amenities_bw = osm_bw.get_pois(custom_filter=custom_filter)\n",
    "parking_amenities_nw = osm_nw.get_pois(custom_filter=custom_filter)\n",
    "\n",
    "parking_amenities_ns.to_csv(\"data/osm/poi/osm_poi_parking_amenities_ns.csv\", index=False)\n",
    "parking_amenities_bw.to_csv(\"data/osm/poi/osm_poi_parking_amenities_bw.csv\", index=False)\n",
    "parking_amenities_nw.to_csv(\"data/osm/poi/osm_poi_parking_amenities_nw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traffic indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_filter = {'type': ['traffic'],\n",
    "                \"highway\": ['traffic_signals','mini_roundabout',\n",
    "                            'stop', 'crossing', 'level_crossing',\n",
    "                            'ford', 'motorway_junction', 'turning_circle',\n",
    "                            'speed_camera', 'street_lamp']\n",
    "                }\n",
    "\n",
    "traffic_indicators_ns = osm_ns.get_pois(custom_filter=custom_filter)\n",
    "traffic_indicators_bw = osm_bw.get_pois(custom_filter=custom_filter)\n",
    "traffic_indicators_nw = osm_nw.get_pois(custom_filter=custom_filter)\n",
    "\n",
    "traffic_indicators_ns.to_csv(\"data/osm/poi/osm_poi_traffic_indicators_ns.csv\", index=False)\n",
    "traffic_indicators_bw.to_csv(\"data/osm/poi/osm_poi_traffic_indicators_bw.csv\", index=False)\n",
    "traffic_indicators_nw.to_csv(\"data/osm/poi/osm_poi_traffic_indicators_nw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSM data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load OSM data from NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_routes_nl = pd.read_csv(\"data/osm/poi/osm_poi_train_routes.csv\")\n",
    "metro_routes_nl = pd.read_csv(\"data/osm/poi/osm_poi_metro_routes.csv\")\n",
    "bus_routes_nl = pd.read_csv(\".data/osm/poi/osm_poi_bus_routes.csv\")\n",
    "bike_routes_nl = pd.read_csv(\"data/osm/poi/osm_poi_bike_routes.csv\")\n",
    "tram_routes_nl = pd.read_csv(\"data/osm/poi/osm_poi_tram_routes.csv\")\n",
    "\n",
    "train_stops_nl = pd.read_csv('data/osm/poi/osm_poi_train_stops.csv')\n",
    "tram_stops_nl = pd.read_csv('data/osm/poi/osm_poi_tram_stops.csv')\n",
    "metro_stops_nl = pd.read_csv('data/osm/poi/osm_poi_metro_stops.csv')\n",
    "bus_stops_nl = pd.read_csv('data/osm/poi/osm_poi_bus_stops.csv')\n",
    "\n",
    "parking_amenities_nl = pd.read_csv('data/osm/poi/osm_poi_parking_amenities.csv')\n",
    "\n",
    "traffic_indicators_nl = pd.read_csv(\".data/osm/poi/osm_poi_traffic_indicators.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load OSM data from Germany (Niedersachsen, Baden-Wuertemberg, Nordrhein-Westfalen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_routes_ns = pd.read_csv(\"data/osm/poi/osm_poi_train_routes_ns.csv\")\n",
    "\n",
    "metro_routes_ns = pd.read_csv(\"data/osm/poi/osm_poi_metro_routes_ns.csv\")\n",
    "bus_routes_ns = pd.read_csv(\"data/osm/poi/osm_poi_bus_routes_ns.csv\")\n",
    "bike_routes_ns = pd.read_csv(\"data/osm/poi/osm_poi_bike_routes_ns.csv\")\n",
    "tram_routes_ns = pd.read_csv(\"data/osm/poi/osm_poi_tram_routes_ns.csv\")\n",
    "\n",
    "train_stops_ns = pd.read_csv('data/osm/poi/osm_poi_train_stops_ns.csv')\n",
    "tram_stops_ns = pd.read_csv('data/osm/poi/osm_poi_tram_stops_ns.csv')\n",
    "metro_stops_ns = pd.read_csv('data/osm/poi/osm_poi_metro_stops_ns.csv')\n",
    "bus_stops_ns = pd.read_csv('data/osm/poi/osm_poi_bus_stops_ns.csv')\n",
    "\n",
    "parking_amenities_ns = pd.read_csv('data/osm/poi/osm_poi_parking_amenities_ns.csv')\n",
    "\n",
    "traffic_indicators_ns = pd.read_csv(\"data/osm/poi/osm_poi_traffic_indicators_ns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_routes_bw = pd.read_csv(\".data/osm/poi/osm_poi_train_routes_bw.csv\")\n",
    "\n",
    "metro_routes_bw = pd.read_csv(\".data/osm/poi/osm_poi_metro_routes_bw.csv\")\n",
    "bus_routes_bw = pd.read_csv(\"data/osm/poi/osm_poi_bus_routes_bw.csv\")\n",
    "bike_routes_bw = pd.read_csv(\"data/osm/poi/osm_poi_bike_routes_bw.csv\")\n",
    "tram_routes_bw = pd.read_csv(\".data/osm/poi/osm_poi_tram_routes_bw.csv\")\n",
    "\n",
    "train_stops_bw = pd.read_csv('data/osm/poi/osm_poi_train_stops_bw.csv')\n",
    "tram_stops_bw = pd.read_csv('data/osm/poi/osm_poi_tram_stops_bw.csv')\n",
    "metro_stops_bw = pd.read_csv('dataosm/poi/osm_poi_metro_stops_bw.csv')\n",
    "bus_stops_bw = pd.read_csv('data/osm/poi/osm_poi_bus_stops_bw.csv')\n",
    "\n",
    "parking_amenities_bw = pd.read_csv('data/osm/poi/osm_poi_parking_amenities_bw.csv')\n",
    "\n",
    "traffic_indicators_bw = pd.read_csv(\"data/osm/poi/osm_poi_traffic_indicators_bw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_routes_nw = pd.read_csv(\"data/osm/poi/osm_poi_train_routes_ns.csv\")\n",
    "metro_routes_nw = pd.read_csv(\"data/osm/poi/osm_poi_metro_routes_nw.csv\")\n",
    "bus_routes_nw = pd.read_csv(\"data/osm/poi/osm_poi_bus_routes_nw.csv\")\n",
    "bike_routes_nw = pd.read_csv(\"data/osm/poi/osm_poi_bike_routes_nw.csv\")\n",
    "tram_routes_nw = pd.read_csv(\"data/osm/poi/osm_poi_tram_routes_nw.csv\")\n",
    "\n",
    "train_stops_nw = pd.read_csv('data/osm/poi/osm_poi_train_stops_nw.csv')\n",
    "tram_stops_nw = pd.read_csv('data/osm/poi/osm_poi_tram_stops_nw.csv')\n",
    "metro_stops_nw = pd.read_csv('data/osm/poi/osm_poi_metro_stops_nw.csv')\n",
    "bus_stops_nw = pd.read_csv('data/osm/poi/osm_poi_bus_stops_nw.csv')\n",
    "\n",
    "parking_amenities_nw = pd.read_csv('data/osm/poi/osm_poi_parking_amenities_nw.csv')\n",
    "\n",
    "traffic_indicators_nw = pd.read_csv(\"data/osm/poi/osm_poi_traffic_indicators_nw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine different regions/countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_routes = pd.concat([train_routes_nl, train_routes_ns, train_routes_bw, train_routes_nw],\n",
    "                         ignore_index=True)\n",
    "metro_routes = pd.concat([metro_routes_nl, metro_routes_ns, metro_routes_bw, metro_routes_nw],\n",
    "                         ignore_index=True)\n",
    "bus_routes = pd.concat([bus_routes_nl, bus_routes_ns, bus_routes_bw, bus_routes_nw],\n",
    "                       ignore_index = True)\n",
    "bike_routes = pd.concat([bike_routes_nl, bike_routes_ns, bike_routes_bw, bike_routes_nw],\n",
    "                       ignore_index = True)\n",
    "tram_routes = pd.concat([tram_routes_nl, tram_routes_ns, tram_routes_bw, tram_routes_nw],\n",
    "                       ignore_index = True)\n",
    "\n",
    "train_stops = pd.concat([train_stops_nl, train_stops_ns, train_stops_bw, train_stops_nw],\n",
    "                         ignore_index=True)\n",
    "tram_stops = pd.concat([tram_stops_nl, tram_stops_ns, tram_stops_bw, tram_stops_nw],\n",
    "                         ignore_index=True)\n",
    "metro_stops = pd.concat([metro_stops_nl, metro_stops_ns, metro_stops_bw, metro_stops_nw],\n",
    "                         ignore_index=True)\n",
    "bus_stops = pd.concat([bus_stops_nl, bus_stops_ns, bus_stops_bw, bus_stops_nw],\n",
    "                         ignore_index=True)\n",
    "\n",
    "parking_amenities = pd.concat([parking_amenities_nl, parking_amenities_ns, parking_amenities_bw, parking_amenities_nw],\n",
    "                         ignore_index=True)\n",
    "\n",
    "traffic_indicators = pd.concat([traffic_indicators_nl, traffic_indicators_ns, traffic_indicators_bw, traffic_indicators_nw],\n",
    "                         ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine public transport stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stops['transport_type'] = 'train'\n",
    "tram_stops['transport_type'] = 'tram'\n",
    "metro_stops['transport_type'] = 'metro'\n",
    "bus_stops['transport_type'] = 'bus'\n",
    "\n",
    "public_transport_stops = pd.concat([train_stops, tram_stops,\n",
    "                                    metro_stops, bus_stops], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_routes['geometry'] = metro_routes['geometry'].apply(wkt.loads)\n",
    "metro_routes_gdf = gpd.GeoDataFrame(metro_routes, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "train_routes['geometry'] = train_routes['geometry'].apply(wkt.loads)\n",
    "train_routes_gdf = gpd.GeoDataFrame(train_routes, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "bus_routes['geometry'] = bus_routes['geometry'].apply(wkt.loads)\n",
    "bus_routes_gdf = gpd.GeoDataFrame(bus_routes, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "bike_routes['geometry'] = bike_routes['geometry'].apply(wkt.loads)\n",
    "bike_routes_gdf = gpd.GeoDataFrame(bike_routes, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "tram_routes['geometry'] = tram_routes['geometry'].apply(wkt.loads)\n",
    "tram_routes_gdf = gpd.GeoDataFrame(tram_routes, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "train_stops['geometry'] = train_stops['geometry'].apply(wkt.loads)\n",
    "train_stops_gdf = gpd.GeoDataFrame(train_stops, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "tram_stops['geometry'] = tram_stops['geometry'].apply(wkt.loads)\n",
    "tram_stops_gdf = gpd.GeoDataFrame(tram_stops, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "metro_stops['geometry'] = metro_stops['geometry'].apply(wkt.loads)\n",
    "metro_stops_gdf = gpd.GeoDataFrame(metro_stops, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "bus_stops['geometry'] = bus_stops['geometry'].apply(wkt.loads)\n",
    "bus_stops_gdf = gpd.GeoDataFrame(bus_stops, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "parking_amenities['geometry'] = parking_amenities['geometry'].apply(wkt.loads)\n",
    "parking_amenities_gdf = gpd.GeoDataFrame(parking_amenities, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "traffic_indicators['geometry'] = traffic_indicators['geometry'].apply(wkt.loads)\n",
    "traffic_indicators_gdf = gpd.GeoDataFrame(traffic_indicators, geometry='geometry', crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPS data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GPS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Read Data\n",
    "chunk = pd.read_csv(\"data/events_uu_feat.csv\", chunksize=10000) # prepped event data\n",
    "events = pd.concat(chunk)\n",
    "\n",
    "chunk = pd.read_csv(\"data/geolocations_uu_feat.csv\", chunksize=10000) # prepped geolocations data\n",
    "geolocations = pd.concat(chunk)\n",
    "\n",
    "# Re-enable warnings\n",
    "# warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert geolocations to geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GeoDataFrame for the geolocations data\n",
    "gps_points = [Point(lon, lat) for lon, lat in zip(geolocations['lon'], geolocations['lat'])]\n",
    "geolocations_gdf = gpd.GeoDataFrame(geolocations, geometry=gps_points, crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create event-level linestrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create geometry for each group\n",
    "def create_geometry(group):\n",
    "    points = list(zip(group['lon'], group['lat']))\n",
    "    if len(points) > 1:\n",
    "        return LineString(points)\n",
    "    else:\n",
    "        return Point(points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by event_id and create geometries\n",
    "grouped_geolocations = geolocations.groupby('event_id')\n",
    "geometries = grouped_geolocations.apply(create_geometry, include_groups = False)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "events_gdf = gpd.GeoDataFrame(geometries, columns=['geometry'], crs=\"EPSG:4326\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximity to public transport stations and parking amenities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Haversine great-circle distance between points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance_vectorized(lat1, lon1, lat2, lon2):\n",
    "\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = np.radians(lat1[:, np.newaxis])  # Shape (n, 1)\n",
    "    lon1 = np.radians(lon1[:, np.newaxis])  # Shape (n, 1)\n",
    "    lat2 = np.radians(lat2)                 # Shape (m,)\n",
    "    lon2 = np.radians(lon2)                 # Shape (m,)\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    return R * c  # Distance in kilometers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute nearest station/amenity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_nearest(df_gps, df_osm, label, n1=5, n2=5):\n",
    "    \"\"\"\n",
    "    Calculate the average distance to the nearest parking amenity for the first n1 and last n2 GPS points, respectively.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Prepare the station coordinates\n",
    "    station_lats = df_osm['lat'].values\n",
    "    station_lons = df_osm['lon'].values\n",
    "\n",
    "    # Loop over each user and track\n",
    "    for (user_id, event_id), trip_data in df_gps.groupby(['user_id', 'event_id']):\n",
    "        # Select first and last points\n",
    "        first_points = trip_data[trip_data['accuracy']<=20].head(n1)\n",
    "        last_points = trip_data[trip_data['accuracy']<=20].tail(n2)\n",
    "\n",
    "        # if empty, use the point with lowest accuracy from the first n1*2 observations\n",
    "        if first_points.empty:\n",
    "            first_points = trip_data.head(n1*2)\n",
    "            first_points = first_points[first_points['accuracy']==first_points['accuracy'].min()]\n",
    "            last_points = trip_data.tail(n2*2)\n",
    "            last_points = last_points[last_points['accuracy']==last_points['accuracy'].min()]\n",
    "\n",
    "        # Extract GPS points for the first and last sections\n",
    "        gps_lats_start = first_points['lat'].values\n",
    "        gps_lons_start = first_points['lon'].values\n",
    "        gps_lats_end = last_points['lat'].values\n",
    "        gps_lons_end = last_points['lon'].values\n",
    "        \n",
    "        # Calculate distances for the start points (to nearest station)\n",
    "        distances_start = haversine_distance_vectorized(gps_lats_start, gps_lons_start, station_lats, station_lons)\n",
    "        nearest_distances_start = distances_start.min(axis=1)  # Nearest station distance for each point\n",
    "        avg_distance_start = nearest_distances_start.mean()  # Average distance for the first n1 points\n",
    "\n",
    "        # Calculate distances for the end points (to nearest station)\n",
    "        distances_end = haversine_distance_vectorized(gps_lats_end, gps_lons_end, station_lats, station_lons)\n",
    "        nearest_distances_end = distances_end.min(axis=1)  # Nearest station distance for each point\n",
    "        avg_distance_end = nearest_distances_end.mean()  # Average distance for the last n2 points\n",
    "\n",
    "        results.append({\n",
    "            'event_id': event_id,\n",
    "            f'dist_to_{label}_start': avg_distance_start,\n",
    "            f'dist_to_{label}_end': avg_distance_end\n",
    "        })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximity to parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter parking amenity\n",
    "amenity_parking = parking_amenities[parking_amenities['amenity']=='parking']\n",
    "amenity_parking = amenity_parking.dropna(subset=['lat', 'lon'])\n",
    "\n",
    "amenity_parking_prox = calc_nearest(geolocations, amenity_parking, \"parking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximity to parking entrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter parking entrance amenity\n",
    "amenity_parking_entrance = parking_amenities[parking_amenities['amenity']=='parking_entrance']\n",
    "amenity_parking_entrance = amenity_parking_entrance.dropna(subset=['lat', 'lon'])\n",
    "\n",
    "amenity_parking_entrance_prox = calc_nearest(geolocations, amenity_parking_entrance, \"parking_entrance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximity to charging station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter charging station amenity\n",
    "amenity_charging_station = parking_amenities[parking_amenities['amenity']=='charging_station']\n",
    "amenity_charging_station = amenity_charging_station.dropna(subset=['lat', 'lon'])\n",
    "\n",
    "amenity_charging_station_prox = calc_nearest(geolocations, amenity_charging_station, \"charging_station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximity to bike parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amenity_bike_parking = parking_amenities[parking_amenities['amenity']=='bicycle_parking']\n",
    "amenity_bike_parking = amenity_bike_parking.dropna(subset=['lat', 'lon'])\n",
    "\n",
    "amenity_bike_parking_prox = calc_nearest(geolocations, amenity_bike_parking, \"bike_parking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximity to bike rental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amenity_bike_rental = parking_amenities[parking_amenities['amenity']=='bicycle_rental']\n",
    "amenity_bike_rental = amenity_bike_rental.dropna(subset=['lat', 'lon'])\n",
    "\n",
    "amenity_bike_rental_prox = calc_nearest(geolocations, amenity_bike_rental, \"bike_rental\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine and save parking amenities proximity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes \n",
    "parking_amenities_prox = pd.merge(amenity_parking_prox, amenity_parking_entrance_prox, on=['event_id'], how=\"inner\", suffixes=('_df1', '_df2'))\n",
    "parking_amenities_prox = pd.merge(parking_amenities_prox, amenity_charging_station_prox, on=['event_id'], how=\"inner\", suffixes=('_df1', '_df2'))\n",
    "parking_amenities_prox = pd.merge(parking_amenities_prox, amenity_bike_parking_prox, on=['event_id'], how=\"inner\", suffixes=('_df1', '_df2'))\n",
    "parking_amenities_prox = pd.merge(parking_amenities_prox, amenity_bike_rental_prox, on=['event_id'], how=\"inner\", suffixes=('_df1', '_df2'))\n",
    "\n",
    "# Write the dataframe to a CSV file\n",
    "parking_amenities_prox.to_csv(\"osm_parking_amenities_prox_uu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximity to train station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_station_prox = calc_nearest(geolocations, train_stops, \"train_station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximity to tram station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tram_station_prox = calc_nearest(geolocations, tram_stops, \"tram_station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximity to bus station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_station_prox = calc_nearest(geolocations, bus_stops, \"bus_station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximity to metro station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_station_prox = calc_nearest(geolocations, metro_stops, \"metro_station\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine and save public transport station proximity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes \n",
    "public_transport_stations_prox = pd.merge(train_station_prox, tram_station_prox, on=['event_id'], how=\"inner\", suffixes=('_df1', '_df2'))\n",
    "public_transport_stations_prox = pd.merge(public_transport_stations_prox, bus_station_prox, on=['event_id'], how=\"inner\", suffixes=('_df1', '_df2'))\n",
    "public_transport_stations_prox = pd.merge(public_transport_stations_prox, metro_station_prox, on=['event_id'], how=\"inner\", suffixes=('_df1', '_df2'))\n",
    "\n",
    "# Write the dataframe to a CSV file\n",
    "public_transport_stations_prox.to_csv(\"osm_public_transport_stations_prox_uu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count public transport route/station and traffic indicator overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create buffer around OSM and GPS geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 10m and 20m buffer to GPS data (event-level geometries)\n",
    "events_gdf = events_gdf.to_crs(epsg=3395)\n",
    "events_gdf['buffered_geometry_10'] = events_gdf['geometry'].apply(lambda x: x.buffer(10))\n",
    "events_gdf['buffered_geometry_20'] = events_gdf['geometry'].apply(lambda x: x.buffer(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add buffer (10m) to OSM public transport (& bike) routes\n",
    "train_routes_gdf = train_routes_gdf.to_crs(epsg=3395)\n",
    "train_routes_gdf['buffered_geometry_10'] = train_routes_gdf['geometry'].apply(lambda x: x.buffer(10))\n",
    "tram_routes_gdf = tram_routes_gdf.to_crs(epsg=3395)\n",
    "tram_routes_gdf['buffered_geometry_10'] = tram_routes_gdf['geometry'].apply(lambda x: x.buffer(10))\n",
    "bus_routes_gdf = bus_routes_gdf.to_crs(epsg=3395)\n",
    "bus_routes_gdf['buffered_geometry_10'] = bus_routes_gdf['geometry'].apply(lambda x: x.buffer(10))\n",
    "metro_routes_gdf = metro_routes_gdf.to_crs(epsg=3395)\n",
    "metro_routes_gdf['buffered_geometry_10'] = metro_routes_gdf['geometry'].apply(lambda x: x.buffer(10))\n",
    "bike_routes_gdf = bike_routes_gdf.to_crs(epsg=3395)\n",
    "bike_routes_gdf['buffered_geometry_10'] = bike_routes_gdf['geometry'].apply(lambda x: x.buffer(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add buffer (20m) to OSM public transport stations\n",
    "train_stops_gdf = train_stops_gdf.to_crs(epsg=3395)\n",
    "train_stops_gdf['buffered_geometry_20'] = train_stops_gdf['geometry'].apply(lambda x: x.buffer(20))\n",
    "tram_stops_gdf = tram_stops_gdf.to_crs(epsg=3395)\n",
    "tram_stops_gdf['buffered_geometry_20'] = tram_stops_gdf['geometry'].apply(lambda x: x.buffer(20))\n",
    "bus_stops_gdf = bus_stops_gdf.to_crs(epsg=3395)\n",
    "bus_stops_gdf['buffered_geometry_20'] = bus_stops_gdf['geometry'].apply(lambda x: x.buffer(20))\n",
    "metro_stops_gdf = metro_stops_gdf.to_crs(epsg=3395)\n",
    "metro_stops_gdf['buffered_geometry_20'] = metro_stops_gdf['geometry'].apply(lambda x: x.buffer(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add buffer (10m) to OSM traffic indicators\n",
    "traffic_indicators_gdf = traffic_indicators_gdf.to_crs(epsg=3395)\n",
    "traffic_indicators_gdf['buffered_geometry_10'] = traffic_indicators_gdf['geometry'].apply(lambda x: x.buffer(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total area from events\n",
    "events_gdf['total_buffer_area_10'] = events_gdf['buffered_geometry_10'].area\n",
    "events_gdf['total_buffer_area_20'] = events_gdf['buffered_geometry_20'].area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count intersections with public transport & bike routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for train route intersection\n",
    "joined_train_routes = gpd.sjoin(train_routes_gdf.set_geometry('buffered_geometry_10'),\n",
    "                         events_gdf.set_geometry('buffered_geometry_10'), \n",
    "                         how='inner', \n",
    "                         predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_train_routes = joined_train_routes[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_train',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "matched_train_routes = matched_train_routes.merge(events_gdf[['event_id', 'total_buffer_area_10']], on='event_id', how='left')\n",
    "\n",
    "events_train_routes_counts = matched_train_routes.groupby('event_id').agg(\n",
    "    num_train_routes=('cls_train', 'size'),  # Count the number of metro overlaps\n",
    "    total_buffer_area_10=('total_buffer_area_10', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_train_routes_counts['normalized_num_train_routes'] = (events_train_routes_counts['num_train_routes'] / events_train_routes_counts['total_buffer_area_10'])\n",
    "events_train_routes = events_train_routes_counts.reset_index()\n",
    "events_train_routes = events_train_routes[['event_id','num_train_routes', 'normalized_num_train_routes' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for tram route intersection\n",
    "joined_tram_routes = gpd.sjoin(tram_routes_gdf.set_geometry('buffered_geometry_10'),\n",
    "                         events_gdf.set_geometry('buffered_geometry_10'), \n",
    "                         how='inner', \n",
    "                         predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_tram_routes = joined_tram_routes[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_tram',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "matched_tram_routes = matched_tram_routes.merge(events_gdf[['event_id', 'total_buffer_area_10']], on='event_id', how='left')\n",
    "\n",
    "events_tram_routes_counts = matched_tram_routes.groupby('event_id').agg(\n",
    "    num_tram_routes=('cls_tram', 'size'),  # Count the number of metro overlaps\n",
    "    total_buffer_area_10=('total_buffer_area_10', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_tram_routes_counts['normalized_num_tram_routes'] = (events_tram_routes_counts['num_tram_routes'] / events_tram_routes_counts['total_buffer_area_10'])\n",
    "events_tram_routes = events_tram_routes_counts.reset_index()\n",
    "events_tram_routes = events_tram_routes[['event_id','num_tram_routes', 'normalized_num_tram_routes' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for bus route intersection\n",
    "joined_bus_routes = gpd.sjoin(bus_routes_gdf.set_geometry('buffered_geometry_10'),\n",
    "                         events_gdf.set_geometry('buffered_geometry_10'), \n",
    "                         how='inner', \n",
    "                         predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_bus_routes = joined_bus_routes[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_bus',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "matched_bus_routes = matched_bus_routes.merge(events_gdf[['event_id', 'total_buffer_area_10']], on='event_id', how='left')\n",
    "\n",
    "events_bus_routes_counts = matched_bus_routes.groupby('event_id').agg(\n",
    "    num_bus_routes=('cls_bus', 'size'),  # Count the number of metro overlaps\n",
    "    total_buffer_area_10=('total_buffer_area_10', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_bus_routes_counts['normalized_num_bus_routes'] = (events_bus_routes_counts['num_bus_routes'] / events_bus_routes_counts['total_buffer_area_10'])\n",
    "events_bus_routes = events_bus_routes_counts.reset_index()\n",
    "events_bus_routes = events_bus_routes[['event_id','num_bus_routes', 'normalized_num_bus_routes' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for metro route intersection\n",
    "joined_metro_routes = gpd.sjoin(metro_routes_gdf.set_geometry('buffered_geometry_10'),\n",
    "                         events_gdf.set_geometry('buffered_geometry_10'), \n",
    "                         how='inner', \n",
    "                         predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_metro_routes = joined_metro_routes[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_metro',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "matched_metro_routes = matched_metro_routes.merge(events_gdf[['event_id', 'total_buffer_area_10']], on='event_id', how='left')\n",
    "\n",
    "events_metro_routes_counts = matched_metro_routes.groupby('event_id').agg(\n",
    "    num_metro_routes=('cls_metro', 'size'),  # Count the number of metro overlaps\n",
    "    total_buffer_area_10=('total_buffer_area_10', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_metro_routes_counts['normalized_num_metro_routes'] = (events_metro_routes_counts['num_metro_routes'] / events_metro_routes_counts['total_buffer_area_10'])\n",
    "events_metro_routes = events_metro_routes_counts.reset_index()\n",
    "events_metro_routes = events_metro_routes[['event_id','num_metro_routes', 'normalized_num_metro_routes' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for bike route intersection\n",
    "joined_bike_routes = gpd.sjoin(bike_routes_gdf.set_geometry('buffered_geometry_10'),\n",
    "                         events_gdf.set_geometry('buffered_geometry_10'), \n",
    "                         how='inner', \n",
    "                         predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_bike_routes = joined_bike_routes[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_bike',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "matched_bike_routes = matched_bike_routes.merge(events_gdf[['event_id', 'total_buffer_area_10']], on='event_id', how='left')\n",
    "\n",
    "events_bike_routes_counts = matched_bike_routes.groupby('event_id').agg(\n",
    "    num_bike_routes=('cls_bike', 'size'),  # Count the number of metro overlaps\n",
    "    total_buffer_area_10=('total_buffer_area_10', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_bike_routes_counts['normalized_num_bike_routes'] = (events_bike_routes_counts['num_bike_routes'] / events_bike_routes_counts['total_buffer_area_10'])\n",
    "events_bike_routes = events_bike_routes_counts.reset_index()\n",
    "events_bike_routes = events_bike_routes[['event_id','num_bike_routes', 'normalized_num_bike_routes' ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save public transport route count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dataframe to a CSV file\n",
    "events_metro_routes.to_csv(\"osm_metro_route_counts_uu.csv\", index=False)\n",
    "events_train_routes.to_csv(\"osm_train_route_counts_uu.csv\", index=False)\n",
    "events_tram_routes.to_csv(\"osm_tram_route_counts_uu.csv\", index=False)\n",
    "events_bus_routes.to_csv(\"osm_bus_route_counts_uu.csv\", index=False)\n",
    "events_bike_routes.to_csv(\"osm_bike_route_counts_uu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count intersections with public transport stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for train station intersection\n",
    "joined_train_stations = gpd.sjoin(train_stops_gdf.set_geometry('buffered_geometry_20'), \n",
    "                                  events_gdf.set_geometry('buffered_geometry_20'), \n",
    "                                  how='inner', \n",
    "                                  predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_train_stations = joined_train_stations[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_train',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "\n",
    "matched_train_stations = matched_train_stations.merge(events_gdf[['event_id', 'total_buffer_area_20']], on='event_id', how='left')\n",
    "\n",
    "events_train_stations_counts = matched_train_stations.groupby('event_id').agg(\n",
    "    num_train_stations=('cls_train', 'size'),  \n",
    "    total_buffer_area_20=('total_buffer_area_20', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_train_stations_counts['normalized_num_train_stations'] = (events_train_stations_counts['num_train_stations'] / events_train_stations_counts['total_buffer_area_20'])\n",
    "events_train_stations = events_train_stations_counts.reset_index()\n",
    "events_train_stations = events_train_stations[['event_id','num_train_stations', 'normalized_num_train_stations' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for tram station intersection\n",
    "joined_tram_stations = gpd.sjoin(tram_stops_gdf.set_geometry('buffered_geometry_20'), \n",
    "                                  events_gdf.set_geometry('buffered_geometry_20'), \n",
    "                                  how='inner', \n",
    "                                  predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_tram_stations = joined_tram_stations[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_tram',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "\n",
    "matched_tram_stations = matched_tram_stations.merge(events_gdf[['event_id', 'total_buffer_area_20']], on='event_id', how='left')\n",
    "\n",
    "events_tram_stations_counts = matched_tram_stations.groupby('event_id').agg(\n",
    "    num_tram_stations=('cls_tram', 'size'),  \n",
    "    total_buffer_area_20=('total_buffer_area_20', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_tram_stations_counts['normalized_num_tram_stations'] = (events_tram_stations_counts['num_tram_stations'] / events_tram_stations_counts['total_buffer_area_20'])\n",
    "events_tram_stations = events_tram_stations_counts.reset_index()\n",
    "events_tram_stations = events_tram_stations[['event_id','num_tram_stations', 'normalized_num_tram_stations' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for bus station intersection\n",
    "joined_bus_stations = gpd.sjoin(bus_stops_gdf.set_geometry('buffered_geometry_20'), \n",
    "                                  events_gdf.set_geometry('buffered_geometry_20'), \n",
    "                                  how='inner', \n",
    "                                  predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_bus_stations = joined_bus_stations[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_bus',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "\n",
    "matched_bus_stations = matched_bus_stations.merge(events_gdf[['event_id', 'total_buffer_area_20']], on='event_id', how='left')\n",
    "\n",
    "events_bus_stations_counts = matched_bus_stations.groupby('event_id').agg(\n",
    "    num_bus_stations=('cls_bus', 'size'),  \n",
    "    total_buffer_area_20=('total_buffer_area_20', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_bus_stations_counts['normalized_num_bus_stations'] = (events_bus_stations_counts['num_bus_stations'] / events_bus_stations_counts['total_buffer_area_20'])\n",
    "events_bus_stations = events_bus_stations_counts.reset_index()\n",
    "events_bus_stations = events_bus_stations[['event_id','num_bus_stations', 'normalized_num_bus_stations' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for metro station intersection\n",
    "joined_metro_stations = gpd.sjoin(metro_stops_gdf.set_geometry('buffered_geometry_20'), \n",
    "                                  events_gdf.set_geometry('buffered_geometry_20'), \n",
    "                                  how='inner', \n",
    "                                  predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_metro_stations = joined_metro_stations[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_metro',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "\n",
    "matched_metro_stations = matched_metro_stations.merge(events_gdf[['event_id', 'total_buffer_area_20']], on='event_id', how='left')\n",
    "\n",
    "events_metro_stations_counts = matched_metro_stations.groupby('event_id').agg(\n",
    "    num_metro_stations=('cls_metro', 'size'),  \n",
    "    total_buffer_area_20=('total_buffer_area_20', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_metro_stations_counts['normalized_num_metro_stations'] = (events_metro_stations_counts['num_metro_stations'] / events_metro_stations_counts['total_buffer_area_20'])\n",
    "events_metro_stations = events_metro_stations_counts.reset_index()\n",
    "events_metro_stations = events_metro_stations[['event_id','num_metro_stations', 'normalized_num_metro_stations' ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save public transport station count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dataframe to a CSV file\n",
    "events_metro_stations.to_csv(\"osm_metro_station_counts_uu.csv\", index=False)\n",
    "events_train_stations.to_csv(\"osm_train_station_counts_uu.csv\", index=False)\n",
    "events_tram_stations.to_csv(\"osm_tram_station_counts_uu.csv\", index=False)\n",
    "events_bus_stations.to_csv(\"osm_bus_station_counts_uu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count intersections with traffic indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter osm traffic indicators for crossings\n",
    "traffic_crossing_gdf = traffic_indicators_gdf[traffic_indicators_gdf['highway']=='crossing']\n",
    "\n",
    "# check for crossing intersections\n",
    "joined_traffic_crossing = gpd.sjoin(traffic_crossing_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   events_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   how='inner', \n",
    "                   predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_traffic_crossing = joined_traffic_crossing[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_crossing',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "\n",
    "matched_traffic_crossing = matched_traffic_crossing.merge(events_gdf[['event_id', 'total_buffer_area_10']], on='event_id', how='left')\n",
    "\n",
    "events_traffic_crossing_counts = matched_traffic_crossing.groupby('event_id').agg(\n",
    "    num_crossing=('cls_crossing', 'size'),  # Count the number of junctions\n",
    "    total_buffer_area_10=('total_buffer_area_10', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_traffic_crossing_counts['normalized_num_crossings'] = (events_traffic_crossing_counts['num_crossing'] / events_traffic_crossing_counts['total_buffer_area_10'])\n",
    "events_traffic_crossing = events_traffic_crossing_counts.reset_index()\n",
    "\n",
    "events_traffic_crossing = events_traffic_crossing[['event_id','num_crossing', 'normalized_num_crossings' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter osm traffic indicators for motorway junctions\n",
    "traffic_motorway_junction_gdf = traffic_indicators_gdf[traffic_indicators_gdf['highway']=='motorway_junction']\n",
    "\n",
    "# check for crossing intersections\n",
    "joined_traffic_motorway_junction = gpd.sjoin(traffic_motorway_junction_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   events_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   how='inner', \n",
    "                   predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_traffic_motorway_junction = joined_traffic_motorway_junction[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_motorway_junction',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "\n",
    "matched_traffic_motorway_junction = matched_traffic_motorway_junction.merge(events_gdf[['event_id', 'total_buffer_area_10']], on='event_id', how='left')\n",
    "\n",
    "events_traffic_motorway_junction_counts = matched_traffic_motorway_junction.groupby('event_id').agg(\n",
    "    num_motorway_junction=('cls_motorway_junction', 'size'),  # Count the number of junctions\n",
    "    total_buffer_area_10=('total_buffer_area_10', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_traffic_motorway_junction_counts['normalized_num_motorway_junction'] = (events_traffic_motorway_junction_counts['num_motorway_junction'] / events_traffic_motorway_junction_counts['total_buffer_area_10'])\n",
    "events_traffic_motorway_junction = events_traffic_motorway_junction_counts.reset_index()\n",
    "\n",
    "events_traffic_motorway_junction = events_traffic_motorway_junction[['event_id','num_motorway_junction', 'normalized_num_motorway_junction' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter osm traffic indicators for traffic signals\n",
    "traffic_signals_gdf = traffic_indicators_gdf[traffic_indicators_gdf['highway']=='traffic_signals']\n",
    "\n",
    "# check for crossing intersections\n",
    "joined_traffic_signals = gpd.sjoin(traffic_signals_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   events_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   how='inner', \n",
    "                   predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_traffic_signals = joined_traffic_signals[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_traffic_signals',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "\n",
    "matched_traffic_signals = matched_traffic_signals.merge(events_gdf[['event_id', 'total_buffer_area_10']], on='event_id', how='left')\n",
    "\n",
    "events_traffic_signals_counts = matched_traffic_signals.groupby('event_id').agg(\n",
    "    num_traffic_signals=('cls_traffic_signals', 'size'),  # Count the number of junctions\n",
    "    total_buffer_area_10=('total_buffer_area_10', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_traffic_signals_counts['normalized_num_traffic_signals'] = (events_traffic_signals_counts['num_traffic_signals'] / events_traffic_signals_counts['total_buffer_area_10'])\n",
    "events_traffic_signals = events_traffic_signals_counts.reset_index()\n",
    "\n",
    "events_traffic_signals = events_traffic_signals[['event_id','num_traffic_signals', 'normalized_num_traffic_signals' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter osm traffic indicators for turning circles\n",
    "traffic_turning_circle_gdf = traffic_indicators_gdf[traffic_indicators_gdf['highway']=='turning_circle']\n",
    "\n",
    "# check for crossing intersections\n",
    "joined_traffic_turning_circle = gpd.sjoin(traffic_turning_circle_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   events_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   how='inner', \n",
    "                   predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_traffic_turning_circle = joined_traffic_turning_circle[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_turning_circle',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "\n",
    "matched_traffic_turning_circle = matched_traffic_turning_circle.merge(events_gdf[['event_id', 'total_buffer_area_10']], on='event_id', how='left')\n",
    "\n",
    "events_traffic_turning_circle_counts = matched_traffic_turning_circle.groupby('event_id').agg(\n",
    "    num_turning_circle=('cls_turning_circle', 'size'),  # Count the number of junctions\n",
    "    total_buffer_area_10=('total_buffer_area_10', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_traffic_turning_circle_counts['normalized_num_turning_circle'] = (events_traffic_turning_circle_counts['num_turning_circle'] / events_traffic_turning_circle_counts['total_buffer_area_10'])\n",
    "events_traffic_turning_circle = events_traffic_turning_circle_counts.reset_index()\n",
    "\n",
    "events_traffic_turning_circle = events_traffic_turning_circle[['event_id','num_turning_circle', 'normalized_num_turning_circle' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter osm traffic indicators for stops\n",
    "traffic_stop_gdf = traffic_indicators_gdf[traffic_indicators_gdf['highway']=='stop']\n",
    "\n",
    "# check for crossing intersections\n",
    "joined_traffic_stop = gpd.sjoin(traffic_stop_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   events_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   how='inner', \n",
    "                   predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_traffic_stop = joined_traffic_stop[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_stop',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "\n",
    "matched_traffic_stop = matched_traffic_stop.merge(events_gdf[['event_id', 'total_buffer_area_10']], on='event_id', how='left')\n",
    "\n",
    "events_traffic_stop_counts = matched_traffic_stop.groupby('event_id').agg(\n",
    "    num_stop=('cls_stop', 'size'),  # Count the number of junctions\n",
    "    total_buffer_area_10=('total_buffer_area_10', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_traffic_stop_counts['normalized_num_stop'] = (events_traffic_stop_counts['num_stop'] / events_traffic_stop_counts['total_buffer_area_10'])\n",
    "events_traffic_stop = events_traffic_stop_counts.reset_index()\n",
    "\n",
    "events_traffic_stop = events_traffic_stop[['event_id','num_stop', 'normalized_num_stop' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter osm traffic indicators for speed cameras\n",
    "traffic_speed_camera_gdf = traffic_indicators_gdf[traffic_indicators_gdf['highway']=='speed_camera']\n",
    "\n",
    "# check for crossing intersections\n",
    "joined_traffic_speed_camera = gpd.sjoin(traffic_speed_camera_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   events_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   how='inner', \n",
    "                   predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_traffic_speed_camera = joined_traffic_speed_camera[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_speed_camera',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "\n",
    "matched_traffic_speed_camera = matched_traffic_speed_camera.merge(events_gdf[['event_id', 'total_buffer_area_10']], on='event_id', how='left')\n",
    "\n",
    "events_traffic_speed_camera_counts = matched_traffic_speed_camera.groupby('event_id').agg(\n",
    "    num_speed_camera=('cls_speed_camera', 'size'),  # Count the number of junctions\n",
    "    total_buffer_area_10=('total_buffer_area_10', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_traffic_speed_camera_counts['normalized_num_speed_camera'] = (events_traffic_speed_camera_counts['num_speed_camera'] / events_traffic_speed_camera_counts['total_buffer_area_10'])\n",
    "events_traffic_speed_camera = events_traffic_speed_camera_counts.reset_index()\n",
    "\n",
    "events_traffic_speed_camera = events_traffic_speed_camera[['event_id','num_speed_camera', 'normalized_num_speed_camera' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter osm traffic indicators for mini roundabouts\n",
    "traffic_mini_roundabout_gdf = traffic_indicators_gdf[traffic_indicators_gdf['highway']=='mini_roundabout']\n",
    "\n",
    "# check for crossing intersections\n",
    "joined_traffic_mini_roundabout = gpd.sjoin(traffic_mini_roundabout_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   events_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   how='inner', \n",
    "                   predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_traffic_mini_roundabout = joined_traffic_mini_roundabout[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_mini_roundabout',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "\n",
    "matched_traffic_mini_roundabout = matched_traffic_mini_roundabout.merge(events_gdf[['event_id', 'total_buffer_area_10']], on='event_id', how='left')\n",
    "\n",
    "events_traffic_mini_roundabout_counts = matched_traffic_mini_roundabout.groupby('event_id').agg(\n",
    "    num_mini_roundabout=('cls_mini_roundabout', 'size'),  # Count the number of junctions\n",
    "    total_buffer_area_10=('total_buffer_area_10', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_traffic_mini_roundabout_counts['normalized_num_mini_roundabout'] = (events_traffic_mini_roundabout_counts['num_mini_roundabout'] / events_traffic_mini_roundabout_counts['total_buffer_area_10'])\n",
    "events_traffic_mini_roundabout = events_traffic_mini_roundabout_counts.reset_index()\n",
    "\n",
    "events_traffic_mini_roundabout = events_traffic_mini_roundabout[['event_id','num_mini_roundabout', 'normalized_num_mini_roundabout' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter osm traffic indicators for street lamps\n",
    "traffic_street_lamp_gdf = traffic_indicators_gdf[traffic_indicators_gdf['highway']=='street_lamp']\n",
    "\n",
    "# check for crossing intersections\n",
    "joined_traffic_street_lamp = gpd.sjoin(traffic_street_lamp_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   events_gdf.set_geometry('buffered_geometry_10'), \n",
    "                   how='inner', \n",
    "                   predicate='intersects')\n",
    "\n",
    "# Create the output DataFrame\n",
    "matched_traffic_street_lamp = joined_traffic_street_lamp[['geometry_left', 'event_id', 'geometry_right']].rename(columns={\n",
    "    'geometry_left': 'cls_street_lamp',\n",
    "    'geometry_right': 'cls_geometry'\n",
    "})\n",
    "\n",
    "matched_traffic_street_lamp = matched_traffic_street_lamp.merge(events_gdf[['event_id', 'total_buffer_area_10']], on='event_id', how='left')\n",
    "\n",
    "events_traffic_street_lamp_counts = matched_traffic_street_lamp.groupby('event_id').agg(\n",
    "    num_street_lamp=('cls_street_lamp', 'size'),  # Count the number of junctions\n",
    "    total_buffer_area_10=('total_buffer_area_10', 'first')  # Get the total area for each track\n",
    ")\n",
    "\n",
    "# Normalize by total area\n",
    "events_traffic_street_lamp_counts['normalized_num_street_lamp'] = (events_traffic_street_lamp_counts['num_street_lamp'] / events_traffic_street_lamp_counts['total_buffer_area_10'])\n",
    "events_traffic_street_lamp = events_traffic_street_lamp_counts.reset_index()\n",
    "\n",
    "events_traffic_street_lamp = events_traffic_street_lamp[['event_id','num_street_lamp', 'normalized_num_street_lamp' ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save traffic indicator count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dataframe to a CSV file\n",
    "events_traffic_crossing.to_csv(\"osm_traffic_crossing_counts_uu.csv\", index=False)\n",
    "events_traffic_motorway_junction.to_csv(\"osm_traffic_motorway_junction_counts_uu.csv\", index=False)\n",
    "events_traffic_signals.to_csv(\"osm_traffic_signals_counts_uu.csv\", index=False)\n",
    "events_traffic_turning_circle.to_csv(\"osm_traffic_turning_circle_counts_uu.csv\", index=False)\n",
    "events_traffic_stop.to_csv(\"osm_traffic_stop_counts_uu.csv\", index=False)\n",
    "events_traffic_speed_camera.to_csv(\"osm_traffic_speed_camera_counts_uu.csv\", index=False)\n",
    "events_traffic_mini_roundabout.to_csv(\"osm_traffic_mini_roundabout_counts_uu.csv\", index=False)\n",
    "events_traffic_street_lamp.to_csv(\"osm_traffic_street_lamp_counts_uu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance (intersection) to public transport & bike routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_routes_merged = pd.merge(\n",
    "    matched_metro_routes, \n",
    "    events_gdf, \n",
    "    on=\"event_id\", \n",
    "    how=\"inner\")\n",
    "\n",
    "train_routes_merged = pd.merge(\n",
    "    matched_train_routes, \n",
    "    events_gdf, \n",
    "    on=\"event_id\", \n",
    "    how=\"inner\")\n",
    "\n",
    "tram_routes_merged = pd.merge(\n",
    "    matched_tram_routes, \n",
    "    events_gdf, \n",
    "    on=\"event_id\", \n",
    "    how=\"inner\")\n",
    "\n",
    "bus_routes_merged = pd.merge(\n",
    "    matched_bus_routes, \n",
    "    events_gdf, \n",
    "    on=\"event_id\", \n",
    "    how=\"inner\")\n",
    "\n",
    "bike_routes_merged = pd.merge(\n",
    "    matched_bike_routes, \n",
    "    events_gdf, \n",
    "    on=\"event_id\", \n",
    "    how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance to train routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the distance to train track\n",
    "def calculate_distance_to_train(row):\n",
    "    cls_geometry = row['cls_geometry']\n",
    "    cls_train_track = row['cls_train']\n",
    "    \n",
    "    # Extract points from the LineString (cls_geometry)\n",
    "    points = [Point(coord) for coord in cls_geometry.coords]  # Convert each coordinate into a Point\n",
    "    \n",
    "    # Calculate the distance from each point to the nearest point on the cls_train_track\n",
    "    distances = [point.distance(cls_train_track) for point in points]\n",
    "    \n",
    "    # Calculate multiple statistics on the distances\n",
    "    min_distance = min(distances)\n",
    "    max_distance = max(distances)\n",
    "    mean_distance = np.mean(distances)\n",
    "    std_distance = np.std(distances)\n",
    "    \n",
    "    # Return the statistics as a dictionary\n",
    "    return {'min_distance_train': min_distance, \n",
    "            'max_distance_train': max_distance, \n",
    "            'mean_distance_train': mean_distance, \n",
    "            'std_distance_train': std_distance}\n",
    "\n",
    "\n",
    "def apply_distance_train(chunk):\n",
    "    # Apply the function and convert the dictionary output into a DataFrame\n",
    "    results = chunk.apply(calculate_distance_to_train, axis=1)\n",
    "    return pd.DataFrame(results.tolist(), index=chunk.index)\n",
    "\n",
    "# Function to apply calculation in parallel\n",
    "def parallel_calculation_train(df, num_chunks=4):\n",
    "    # Split the DataFrame into chunks\n",
    "    chunk_size = len(df) // num_chunks\n",
    "    chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "    \n",
    "    # Initialize Pool and parallelize the calculation\n",
    "    with Pool(num_chunks) as pool:\n",
    "        result = pool.map(apply_distance_train, chunks)\n",
    "    \n",
    "    # Combine the results\n",
    "    distances = pd.concat(result, axis=0)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "start_time = time.time()\n",
    "\n",
    "#parallel calculation\n",
    "distance_stats_train = parallel_calculation_train(train_routes_merged, num_chunks=1)\n",
    "\n",
    "# Assign the results to separate columns\n",
    "train_routes_merged[['min_distance_train', 'max_distance_train', 'mean_distance_train', 'std_distance_train']] = distance_stats_train\n",
    "\n",
    "# End \n",
    "end_time = time.time()\n",
    "\n",
    "# Output the time taken\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for parallel processing: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance to tram routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the distance to tram track\n",
    "def calculate_distance_to_tram(row):\n",
    "    cls_geometry = row['cls_geometry']\n",
    "    cls_tram_track = row['cls_tram']\n",
    "    \n",
    "    # Extract points from the LineString (cls_geometry)\n",
    "    points = [Point(coord) for coord in cls_geometry.coords]  # Convert each coordinate into a Point\n",
    "    \n",
    "    # Calculate the distance from each point to the nearest point on the cls_metro_track\n",
    "    distances = [point.distance(cls_tram_track) for point in points]\n",
    "    \n",
    "    # Calculate multiple statistics on the distances\n",
    "    min_distance = min(distances)\n",
    "    max_distance = max(distances)\n",
    "    mean_distance = np.mean(distances)\n",
    "    std_distance = np.std(distances)\n",
    "    \n",
    "    # Return the statistics as a dictionary\n",
    "    return {'min_distance_tram': min_distance, \n",
    "            'max_distance_tram': max_distance, \n",
    "            'mean_distance_tram': mean_distance, \n",
    "            'std_distance_tram': std_distance}\n",
    "\n",
    "\n",
    "def apply_distance_tram(chunk):\n",
    "    # Apply the function and convert the dictionary output into a DataFrame\n",
    "    results = chunk.apply(calculate_distance_to_tram, axis=1)\n",
    "    return pd.DataFrame(results.tolist(), index=chunk.index)\n",
    "\n",
    "# Function to apply calculation in parallel\n",
    "def parallel_calculation_tram(df, num_chunks=4):\n",
    "    # Split the DataFrame into chunks\n",
    "    chunk_size = len(df) // num_chunks\n",
    "    chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "    \n",
    "    # Initialize Pool and parallelize the calculation\n",
    "    with Pool(num_chunks) as pool:\n",
    "        result = pool.map(apply_distance_tram, chunks)\n",
    "    \n",
    "    # Combine the results\n",
    "    distances = pd.concat(result, axis=0)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "start_time = time.time()\n",
    "\n",
    "#parallel calculation\n",
    "distance_stats_tram = parallel_calculation_tram(tram_routes_merged, num_chunks=2)\n",
    "\n",
    "# Assign the results to separate columns\n",
    "tram_routes_merged[['min_distance_tram', 'max_distance_tram', 'mean_distance_tram', 'std_distance_tram']] = distance_stats_tram\n",
    "\n",
    "# End \n",
    "end_time = time.time()\n",
    "\n",
    "# Output the time taken\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for parallel processing: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance to bus routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the distance to bus track\n",
    "def calculate_distance_to_bus(row):\n",
    "    cls_geometry = row['cls_geometry']\n",
    "    cls_bus_track = row['cls_bus']\n",
    "    \n",
    "    # Extract points from the LineString (cls_geometry)\n",
    "    points = [Point(coord) for coord in cls_geometry.coords]  # Convert each coordinate into a Point\n",
    "    \n",
    "    # Calculate the distance from each point to the nearest point on the cls_metro_track\n",
    "    distances = [point.distance(cls_bus_track) for point in points]\n",
    "    \n",
    "    # Calculate multiple statistics on the distances\n",
    "    min_distance = min(distances)\n",
    "    max_distance = max(distances)\n",
    "    mean_distance = np.mean(distances)\n",
    "    std_distance = np.std(distances)\n",
    "    \n",
    "    # Return the statistics as a dictionary\n",
    "    return {'min_distance_bus': min_distance, \n",
    "            'max_distance_bus': max_distance, \n",
    "            'mean_distance_bus': mean_distance, \n",
    "            'std_distance_bus': std_distance}\n",
    "\n",
    "\n",
    "def apply_distance_bus(chunk):\n",
    "    # Apply the function and convert the dictionary output into a DataFrame\n",
    "    results = chunk.apply(calculate_distance_to_bus, axis=1)\n",
    "    return pd.DataFrame(results.tolist(), index=chunk.index)\n",
    "\n",
    "# Function to apply calculation in parallel\n",
    "def parallel_calculation_bus(df, num_chunks=4):\n",
    "    # Split the DataFrame into chunks\n",
    "    chunk_size = len(df) // num_chunks\n",
    "    chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "    \n",
    "    # Initialize Pool and parallelize the calculation\n",
    "    with Pool(num_chunks) as pool:\n",
    "        result = pool.map(apply_distance_bus, chunks)\n",
    "    \n",
    "    # Combine the results\n",
    "    distances = pd.concat(result, axis=0)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "start_time = time.time()\n",
    "\n",
    "#parallel calculation\n",
    "distance_stats_bus = parallel_calculation_bus(bus_routes_merged, num_chunks=4)\n",
    "\n",
    "# Assign the results to separate columns\n",
    "bus_routes_merged[['min_distance_bus', 'max_distance_bus', 'mean_distance_bus', 'std_distance_bus']] = distance_stats_bus\n",
    "\n",
    "# End \n",
    "end_time = time.time()\n",
    "\n",
    "# Output the time taken\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for parallel processing: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance to metro routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate the distance to metro track\n",
    "def calculate_distance_to_metro(row):\n",
    "    cls_geometry = row['cls_geometry']\n",
    "    cls_metro_track = row['cls_metro']\n",
    "    \n",
    "    # Extract points from the LineString (cls_geometry)\n",
    "    points = [Point(coord) for coord in cls_geometry.coords]  # Convert each coordinate into a Point\n",
    "    \n",
    "    # Calculate the distance from each point to the nearest point on the cls_metro_track\n",
    "    distances = [point.distance(cls_metro_track) for point in points]\n",
    "    \n",
    "    # Calculate multiple statistics on the distances\n",
    "    min_distance = min(distances)\n",
    "    max_distance = max(distances)\n",
    "    mean_distance = np.mean(distances)\n",
    "    std_distance = np.std(distances)\n",
    "    \n",
    "    # Return the statistics as a dictionary\n",
    "    return {'min_distance_metro': min_distance, \n",
    "            'max_distance_metro': max_distance, \n",
    "            'mean_distance_metro': mean_distance, \n",
    "            'std_distance_metro': std_distance}\n",
    "\n",
    "# Refactor to avoid lambda inside multiprocessing\n",
    "def apply_distance_metro(chunk):\n",
    "    # Apply the function and convert the dictionary output into a DataFrame\n",
    "    results = chunk.apply(calculate_distance_to_metro, axis=1)\n",
    "    return pd.DataFrame(results.tolist(), index=chunk.index)\n",
    "\n",
    "# Function to apply calculation in parallel\n",
    "def parallel_calculation_metro(df, num_chunks=4):\n",
    "    # Split the DataFrame into chunks\n",
    "    chunk_size = len(df) // num_chunks\n",
    "    chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "    \n",
    "    # Initialize Pool and parallelize the calculation\n",
    "    with Pool(num_chunks) as pool:\n",
    "        result = pool.map(apply_distance_metro, chunks)\n",
    "    \n",
    "    # Combine the results\n",
    "    distances = pd.concat(result, axis=0)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "start_time = time.time()\n",
    "\n",
    "#parallel calculation\n",
    "distance_stats_metro = parallel_calculation_metro(metro_routes_merged, num_chunks=1)\n",
    "\n",
    "# Assign the results to separate columns\n",
    "metro_routes_merged[['min_distance_metro', 'max_distance_metro', 'mean_distance_metro', 'std_distance_metro']] = distance_stats_metro\n",
    "\n",
    "# End \n",
    "end_time = time.time()\n",
    "\n",
    "# Output the time taken\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for parallel processing: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance to bike routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the distance to bike track\n",
    "def calculate_distance_to_bike(row):\n",
    "    cls_geometry = row['cls_geometry']\n",
    "    cls_bike_track = row['cls_bike']\n",
    "    \n",
    "    # Extract points from the LineString (cls_geometry)\n",
    "    points = [Point(coord) for coord in cls_geometry.coords]  # Convert each coordinate into a Point\n",
    "    \n",
    "    # Calculate the distance from each point to the nearest point on the cls_metro_track\n",
    "    distances = [point.distance(cls_bike_track) for point in points]\n",
    "    \n",
    "    # Calculate multiple statistics on the distances\n",
    "    min_distance = min(distances)\n",
    "    max_distance = max(distances)\n",
    "    mean_distance = np.mean(distances)\n",
    "    std_distance = np.std(distances)\n",
    "    \n",
    "    # Return the statistics as a dictionary\n",
    "    return {'min_distance_bike': min_distance, \n",
    "            'max_distance_bike': max_distance, \n",
    "            'mean_distance_bike': mean_distance, \n",
    "            'std_distance_bike': std_distance}\n",
    "\n",
    "\n",
    "def apply_distance_bike(chunk):\n",
    "    # Apply the function and convert the dictionary output into a DataFrame\n",
    "    results = chunk.apply(calculate_distance_to_bike, axis=1)\n",
    "    return pd.DataFrame(results.tolist(), index=chunk.index)\n",
    "\n",
    "# Function to apply calculation in parallel\n",
    "def parallel_calculation_bike(df, num_chunks=4):\n",
    "    # Split the DataFrame into chunks\n",
    "    chunk_size = len(df) // num_chunks\n",
    "    chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "    \n",
    "    # Initialize Pool and parallelize the calculation\n",
    "    with Pool(num_chunks) as pool:\n",
    "        result = pool.map(apply_distance_bike, chunks)\n",
    "    \n",
    "    # Combine the results\n",
    "    distances = pd.concat(result, axis=0)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start\n",
    "start_time = time.time()\n",
    "\n",
    "#parallel calculation\n",
    "distance_stats_bike = parallel_calculation_bike(bike_routes_merged, num_chunks=1)\n",
    "\n",
    "# Assign the results to separate columns\n",
    "bike_routes_merged[['min_distance_bike', 'max_distance_bike', 'mean_distance_bike', 'std_distance_bike']] = distance_stats_bike\n",
    "\n",
    "# End \n",
    "end_time = time.time()\n",
    "\n",
    "# Output the time taken\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken for parallel processing: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save distance to routes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save distance to train routes\n",
    "events_train_routes_prox = train_routes_merged[['event_id', 'min_distance_train', 'max_distance_train', 'mean_distance_train', 'std_distance_train']]\n",
    "events_train_routes_prox = events_train_routes_prox.loc[events_train_routes_prox.groupby('event_id')['mean_distance_train'].idxmin()]\n",
    "events_train_routes_prox = events_train_routes_prox.reset_index(drop=True)\n",
    "\n",
    "events_train_routes_prox.to_csv(\"osm_train_route_prox_uu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save distance to tram routes\n",
    "events_tram_routes_prox = tram_routes_merged[['event_id', 'min_distance_tram', 'max_distance_tram', 'mean_distance_tram', 'std_distance_tram']]\n",
    "events_tram_routes_prox = events_tram_routes_prox.loc[events_tram_routes_prox.groupby('event_id')['mean_distance_tram'].idxmin()]\n",
    "events_tram_routes_prox = events_tram_routes_prox.reset_index(drop=True)\n",
    "\n",
    "events_tram_routes_prox.to_csv(\"osm_tram_route_prox_uu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save distance to bus routes\n",
    "events_bus_routes_prox = bus_routes_merged[['event_id', 'min_distance_bus', 'max_distance_bus', 'mean_distance_bus', 'std_distance_bus']]\n",
    "events_bus_routes_prox = events_bus_routes_prox.loc[events_bus_routes_prox.groupby('event_id')['mean_distance_bus'].idxmin()]\n",
    "events_bus_routes_prox = events_bus_routes_prox.reset_index(drop=True)\n",
    "\n",
    "events_bus_routes_prox.to_csv(\"osm_bus_route_prox_uu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save distance to metro routes\n",
    "events_metro_routes_prox = metro_routes_merged[['event_id', 'min_distance_metro', 'max_distance_metro', 'mean_distance_metro', 'std_distance_metro']]\n",
    "events_metro_routes_prox = events_metro_routes_prox.loc[events_metro_routes_prox.groupby('event_id')['mean_distance_metro'].idxmin()]\n",
    "events_metro_routes_prox = events_metro_routes_prox.reset_index(drop=True)\n",
    "\n",
    "events_metro_routes_prox.to_csv(\"osm_metro_route_prox_uu.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save distance to bike routes\n",
    "events_bike_routes_prox = bike_routes_merged[['event_id', 'min_distance_bike', 'max_distance_bike', 'mean_distance_bike', 'std_distance_bike']]\n",
    "events_bike_routes_prox = events_bike_routes_prox.loc[events_bike_routes_prox.groupby('event_id')['mean_distance_bike'].idxmin()]\n",
    "events_bike_routes_prox = events_bike_routes_prox.reset_index(drop=True)\n",
    "\n",
    "events_bike_routes_prox.to_csv(\"osm_bike_route_prox_uu.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
